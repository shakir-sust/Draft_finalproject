---
title: "Data Science Applied to Ag - Final Project - ML"
format:
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Md Shakir Moazzem, Umar Munir
---

# Introduction  

This script outlines parsing open source weather data from Daymet.  

# Setup  

##Loading packages  

The following code chunk will load necessary packages.  

```{r Setup, message=F, warning=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("sf") #to manipulate vector geospatial files
#install.packages("daymetr") #to retrieve data from daymet database website through R
#install.packages("remotes") #to install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") 
#remotes::install_github("ropensci/USAboundariesData")

# Loading packages

library(tidyverse) #need to load "tidyverse" package at first
library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr) #to retrieve data from daymet database website through R
library(remotes)
library(USAboundaries) # for US state boundaries
library(USAboundariesData)

```


## Reading data  

The following code chunk will read the csv files for the 3 training data sets

```{r training data import, message=F, warning=F}

#reading the "training_wrangled.csv" file

tw <- read_csv("../data/training_wrangled.csv") 

# Checking first rows
tw

# Viewing the data frame in a separate tab
View(tw)

```

The following code chunk will rename the the "longitude" column as "lon" and "latitude" column as "lat" in the "tw" object

```{r renaming lon and lat, message=F, warning=F}

tw <- tw %>%
  rename(
    lon = longitude,
    lat = latitude
  )

# Verifying the renaming of "longitude" and "latitude" columns

glimpse(tw)

names(tw)

```

# EDA

## Summary statistics

The following code chunk will conduct EDA of the wrangled data set.

```{r wrangled data EDA, message=F, warning=F }

summary(tw)

```

## Number of unique years

The following code chunk will print the number of unique years.

```{r unique years}

unique(tw$year) %>% #gives a list
  length() #gives the exact number of unique years

```

There are 10 distinct years in the data set.  

## Number of unique sites

The following code chunk will print the number of unique sites.

```{r unique sites}

tw %>%
  distinct(site) %>% #Returns a tibble because it is a tidyverse package
  nrow() #returns number of rows

```

There are 45 distinct sites in the data set.


## Statistical distribution of mean corn yield (Mg/Ha)


```{r fiber strength}

ggplot(data = tw) +
  geom_density(aes(x = mean_yield_mg_ha)) + #to get a density plot for "strength_gtex" [here we don't need a y variable because y is calculated by "geom_density()"]
  geom_rug(aes(x = mean_yield_mg_ha)) #adds a vertical bar at the bottom of the geom_density() to where each observation has happened.

```
Looking at the density plot, the mean is around 9 [from the summary output, "Mean: 9.2025" for "mean_yield_mg_ha"].


## Spatial distribution of mean corn yield (Mg/Ha) in USA

```{r create map of USA and add points, message=F, warning=F}

states <- us_states() %>% 
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #to remove "PR" (Puerto Rico), "AK" (Alaska), and "HI" (Hawaii) from the rows of "states" object i.e., to keep all states that are NOT "PR", "AK", and "HI" #we use "%in%" to filter (i.e., work on) more than one state/entry #if we wanted to filter just 1 state, we would use == sign e.g., !(state_abbr == c("PR"))
  
ggplot() +
  geom_sf(data = states) + #"geom_sf()" is used to plot "sf" object, which we just created above as "states" object; plots all states and territories of USA
  geom_point(data = tw,
             aes(x = lon, #"Longitude" goes on longitude
                 y = lat) #"Latitude" goes on latitude
             )

```


# Open weather data - Daymet  

**Daymet** is an open-source weather data base developed by NASA (https://daymet.ornl.gov).  

> Daymet provides long-term, continuous, gridded estimates of daily weather and climatology variables by interpolating and extrapolating ground-based observations through statistical modeling techniques.  

Data characteristics:    
  - Spatial extent: North America (US, CAN, MEX)  
  - Spatial resolution: **1 km**  
  - Temporal resolution: **daily**  
  - Temporal extent: **1980 to present day**  
  - Variables included:  
    - day length (secs/day)    
    - precipitation (mm/day)  
    - shortwave radiation (W/m2)  
    - snow water equivalent (kg/m2)  
    - maximum air temperature (C)  
    - minimum air temperature (C)  
    - water vapor pressure (Pa)  


## Daymet - one site-year 

The following code chunk downloads the weather data from "Daymet" for only the 1st site and 1st year in the "tw" data frame.

```{r one site-year}

daymet_one <- download_daymet(site = tw$site[[1]], 
                              lat = tw$lat[[1]],
                              lon = tw$lon[[1]],
                              start = tw$year[[1]],
                              end = tw$year[[1]], #we specify the same year that we specifed in "start"
                              simplify = T #to get tidyverse friendly data
                              )

daymet_one 

```


The following code chunk will obtain separate columns for all weather variables for 365 days.

```{r}

#To get separate columns for all weather variables for 365 days, we will pivot_wider() 

daymet_one %>%
  pivot_wider(names_from = measurement,
              values_from = value
              )

```


## Daymet - all site-year 

The following code chunk will filter out site-years having longitudes and latitudes that are outside the scope/range of Daymet database.

```{r}

lat_min <- 14.0 #south
lat_max <- 83.0 #north
lon_min <- -179.0 #west
lon_max <- -52.0 #east

#identifying observations outside Daymet coverage
outside_daymet <- tw %>%
  filter(
    is.na(lat) |
    is.na(lon) |
    lat < lat_min |
    lat > lat_max |
    lon < lon_min |
    lon > lon_max
  )

```


The following code chunk will remove the site-years having longitudes and latitudes outside the range of Daymet database form the original "tw" object

```{r removing outside range lon lat, message=F, warning=F}

tw_clean <- tw %>% 
  anti_join(outside_daymet, 
            by = c("hybrid","year","site")
            )

tw_clean

glimpse(tw_clean)

names(tw_clean)

```


The following code chunk will download the weather data from "Daymet" for all site - years in the "tw" data frame.

```{r}

daymet_all <- tw_clean %>% 
  mutate(weather = pmap(list(.y = year, 
                             .site = site, 
                             .lat = lat, 
                             .lon = lon), 
                        function(.y, .site, .lat, .lon) 
                          download_daymet( 
                            site = .site, #specifying ".site" placeholder for "site = " argument
                            lat = .lat, #specifying ".lat" placeholder for "lat = " argument
                            lon = .lon, #specifying ".lon" placeholder for "lon = " argument
                            start = .y, 
                            end = .y, 
                            simplify = T,
                            silent = T) %>% #end of " download_daymet()" function
                          rename(.year = year,
                                 .site = site) 
                        )) 

head(daymet_all) #To look at the 1st 6 rows

```


The following code chunk will unnest the weather column. 

```{r}

daymet_all_unnest <- daymet_all %>%
  #rename(
    #lon = longitude,
    #lat = latitude
  #) #%>%
  unnest(weather) %>% #To unnest the "weather" column i.e., to unnest the "tibble" dataframes inside all of the cells of the "weather" column and then to bring them back into the main level of the data frame
  pivot_wider(names_from = measurement, #to pivot wide the names from the "measurement" column
              values_from = value) %>% #to pivot wide the values from the "value" column #because we want to change the "measurement" column from long form to wide form
  janitor::clean_names() #to clean and standardize the column names with all lower cases and underscores to fill within words 

daymet_all_unnest

```


# Exporting  

The following code chunk will export the open source weather data pulled from Daymet into a .csv file so that we can reuse it any time without having to download again.  

```{r}

write_csv(daymet_all_unnest,
          "../data/fieldweatherdata.csv"
          )

```





